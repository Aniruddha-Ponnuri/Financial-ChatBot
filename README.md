# Financial Chatbot with Reinforcement Learning ğŸ¤–ğŸ’°

An intelligent financial advisory chatbot powered by AI with **Reinforcement Learning** capabilities that learns from user feedback to continuously improve response quality.

## ğŸŒŸ Features

### Core Capabilities
- ğŸ’¬ **Conversational AI** - Natural language financial advice using Groq LLM (Llama 3.1)
- ğŸ¤– **Reinforcement Learning** - Learns from user feedback to improve responses over time
- ğŸ“Š **Multi-Candidate Generation** - Generates multiple responses and selects the best one
- ğŸ‘ğŸ‘ **User Feedback System** - Rate responses to train the AI
- ğŸ“ˆ **Continuous Learning** - Model updates incrementally with each feedback
- ğŸšï¸ **RL Toggle** - Switch between RL and Standard mode on-the-fly
- ğŸ’¾ **Persistent Storage** - SQLite database for feedback tracking
- ğŸ“ **Conversation History** - Maintains context across messages
- ğŸ‡®ğŸ‡³ **India-Focused** - Financial advice tailored for Indian markets

### Technical Highlights
- âš¡ Fast response generation with Groq API
- ğŸ§  Sentence embedding-based reward model
- ğŸ”„ Incremental learning without full retraining
- ğŸ¨ Modern React UI with feedback controls
- ğŸ“Š Real-time analytics and statistics
- ğŸ”§ Centralized YAML configuration
- ğŸ“‹ Comprehensive logging system

## ğŸ—ï¸ Architecture

```
Frontend (React)          Backend (Flask)           ML Components
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                          
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chatbot.js â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   app.py    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚reward_model â”‚
â”‚  (UI/UX)    â”‚   HTTP   â”‚  (API)      â”‚          â”‚  (Scoring)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚                         â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                          â”‚database.py  â”‚          â”‚sentence    â”‚
                          â”‚(SQLite)     â”‚          â”‚transformersâ”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8+
- Node.js 16+
- npm or yarn
- Groq API Key

### Backend Setup

1. **Clone the repository**
```bash
git clone https://github.com/Aniruddha-Ponnuri/Financial-ChatBot.git
cd Financial-ChatBot
```

2. **Create and activate virtual environment**
```bash
# Using conda
conda create -n chat python=3.10
conda activate chat

# Or using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install Python dependencies**
```bash
pip install flask flask-cors groq python-dotenv sentence-transformers scikit-learn PyYAML
```

4. **Set up environment variables**

Create a `.env` file in the root directory:
```env
GROQ_API_KEY=your_groq_api_key_here
ENVIRONMENT=DEV
```

5. **Configure the bot**

Edit `Backend/bot_config.yaml` to customize:
- System prompts
- RL settings (candidates, temperature)
- Model parameters

### Frontend Setup

1. **Install Node dependencies**
```bash
npm install
```

2. **Verify axios and lucide-react are installed**
```bash
npm install axios lucide-react
```

## ğŸš€ Running the Application

### Start Backend Server

```bash
cd Backend
python app.py
```

The backend will start on `http://localhost:5000`

Expected output:
```
[INFO] Starting Financial Chatbot with RL capabilities
[INFO] RL Mode: Enabled
[INFO] Reward model trained on 0 samples
* Running on http://127.0.0.1:5000
```

### Start Frontend Development Server

In the project directory, you can run:

### `npm start`

Runs the app in the development mode.  
Open [http://localhost:3000](http://localhost:3000) to view it in your browser.

The page will reload when you make changes.

## ğŸ® Using the Application

### User Interface

1. **Header Controls**
   - Toggle between **ğŸ¤– RL Mode** (multiple candidates) and **ğŸ’¬ Standard Mode** (single response)
   - RL toggle is enabled by default

2. **Chat Interface**
   - Type your financial question in the input field
   - Press Enter or click "Ask" button
   - Wait for the response (4-8 seconds in RL mode, 1-2 seconds in standard)

3. **Feedback System**
   - Each bot response shows two buttons: ğŸ‘ (good) and ğŸ‘ (poor)
   - Click to rate the response quality
   - Buttons highlight green (positive) or red (negative) after clicking
   - Feedback is sent to backend to improve the model

4. **RL Badge**
   - Responses generated using RL show a "ğŸ¤– RL" badge
   - Indicates the response was selected from multiple candidates

### Example Workflow

```
1. User: "Should I invest in mutual funds or stocks?"
   â†“
2. System generates 4 candidates (if RL enabled)
   â†“
3. Reward model scores and selects best response
   â†“
4. Bot: "Consider diversification..." [ğŸ¤– RL] [ğŸ‘] [ğŸ‘]
   â†“
5. User clicks ğŸ‘ (positive feedback)
   â†“
6. System learns and improves future responses
```

## ğŸ§ª Testing

### Run the Test Suite

```bash
cd Backend
python test_rl.py
```

This will test:
- Health endpoint
- Standard question handling
- RL-enabled question handling
- Feedback submission
- Statistics retrieval
- Candidate generation

### Manual Testing

1. **Test Standard Mode**
   ```bash
   curl -X POST http://localhost:5000/ask \
     -H "Content-Type: application/json" \
     -d '{"question":"What is diversification?","use_rl":false}'
   ```

2. **Test RL Mode**
   ```bash
   curl -X POST http://localhost:5000/ask \
     -H "Content-Type: application/json" \
     -d '{"question":"What is diversification?","use_rl":true}'
   ```

3. **Submit Feedback**
   ```bash
   curl -X POST http://localhost:5000/feedback \
     -H "Content-Type: application/json" \
     -d '{"question":"What is SIP?","answer":"Systematic Investment Plan...","rating":1}'
   ```

4. **Check Statistics**
   ```bash
   curl http://localhost:5000/feedback/stats
   ```

## ğŸ“Š API Endpoints

| Endpoint | Method | Description | Request Body |
|----------|--------|-------------|--------------|
| `/ask` | POST | Get chatbot response | `{question, history, use_rl}` |
| `/feedback` | POST | Submit user rating | `{question, answer, rating, session_id}` |
| `/feedback/stats` | GET | Get feedback statistics | - |
| `/generate_candidates` | POST | Generate multiple responses | `{question, n}` |
| `/health` | GET | System health check | - |

## âš™ï¸ Configuration

### RL Settings (`Backend/bot_config.yaml`)

```yaml
rl_config:
  enabled: true              # Enable/disable RL
  n_candidates: 4            # Number of responses to generate
  temperature_min: 0.7       # Minimum temperature for diversity
  temperature_max: 1.2       # Maximum temperature for diversity
  reward_model:
    embedding_model: "all-MiniLM-L6-v2"
    min_samples_for_training: 20
```

### Customize Prompts

All prompts are in `Backend/bot_config.yaml`:

```yaml
prompts:
  system_prompt: "You are a financial assistant..."
  general_question_prompt: |
    You are an AI assistant. Answer the following question...
  financial_prompt_template: |
    You are an AI financial assistant. Use your knowledge...
```

## ğŸ“ˆ How RL Works

### Learning Process

1. **Initial State** (No training data)
   - Reward model returns uniform scores
   - Responses selected randomly from candidates

2. **Feedback Collection** (Users rate responses)
   - Positive feedback (ğŸ‘) = rating 1
   - Negative feedback (ğŸ‘) = rating 0
   - Stored in SQLite database

3. **Model Training** (After 20+ samples)
   - Embeddings created: `"Q: {question}\nA: {answer}"`
   - SGD Classifier trained on feedback
   - Model saved to disk

4. **Improved Selection** (Ongoing)
   - Future candidates scored accurately
   - Higher quality responses selected
   - Continuous improvement with more feedback

### Performance Metrics

- **Response Time**: 4-8 seconds (RL) vs 1-2 seconds (Standard)
- **Training Threshold**: 20 minimum samples
- **Optimal Feedback**: 50/50 balance of positive/negative
- **Model Updates**: Incremental (no full retraining)

## ğŸ“ Project Structure

```
Financial-ChatBot/
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ app.py                    # Flask server with RL endpoints
â”‚   â”œâ”€â”€ config.py                 # Configuration loader
â”‚   â”œâ”€â”€ logger.py                 # Custom logging system
â”‚   â”œâ”€â”€ database.py               # SQLite feedback handler
â”‚   â”œâ”€â”€ reward_model.py           # ML reward model
â”‚   â”œâ”€â”€ bot_config.yaml           # Configuration file
â”‚   â”œâ”€â”€ test_rl.py                # Test suite
â”‚   â”œâ”€â”€ feedback.db               # SQLite database (auto-created)
â”‚   â”œâ”€â”€ model_data/               # Saved models
â”‚   â”‚   â””â”€â”€ reward_classifier.pkl
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Chatbot.js                # React chatbot component
â”‚   â”œâ”€â”€ Chatbot.css               # Chatbot styles
â”‚   â”œâ”€â”€ App.js                    # Main app component
â”‚   â””â”€â”€ index.js                  # React entry point
â”œâ”€â”€ public/
â”‚   â””â”€â”€ index.html                # HTML template
â”œâ”€â”€ FRONTEND_RL_GUIDE.md          # Frontend documentation
â”œâ”€â”€ COMPLETE_RL_SUMMARY.md        # Full implementation guide
â”œâ”€â”€ QUICK_REFERENCE.md            # Quick reference card
â”œâ”€â”€ SYSTEM_FLOW_DIAGRAM.md        # Architecture diagram
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ package.json                  # Node dependencies
â””â”€â”€ .env                          # Environment variables
```

## ğŸ”§ Troubleshooting

### Backend Issues

**Problem**: Import errors for ML packages
```bash
# Solution: Install dependencies
pip install sentence-transformers scikit-learn PyYAML
```

**Problem**: Backend won't start
```bash
# Check Python version
python --version  # Should be 3.8+

# Verify GROQ_API_KEY is set
cat .env  # Should contain GROQ_API_KEY=...
```

**Problem**: RL not working
```bash
# Check configuration
cat Backend/bot_config.yaml  # rl_config.enabled should be true

# Check health endpoint
curl http://localhost:5000/health
```

### Frontend Issues

**Problem**: Can't connect to backend
```bash
# Ensure backend is running on port 5000
# Check Chatbot.js has correct URL: http://localhost:5000
```

**Problem**: Feedback buttons not working
```bash
# Check browser console for errors
# Verify axios is installed: npm list axios
```

### Database Issues

**Problem**: Feedback not saving
```bash
# Check database file exists
ls Backend/feedback.db

# Check logs for errors
# Look for: [ERROR] Error saving feedback
```

## ğŸ“š Documentation

- **[Backend RL Guide](Backend/RL_GUIDE.md)** - Complete backend documentation
- **[Frontend RL Guide](FRONTEND_RL_GUIDE.md)** - Frontend integration guide
- **[Complete Summary](COMPLETE_RL_SUMMARY.md)** - Full implementation overview
- **[Quick Reference](QUICK_REFERENCE.md)** - Quick start guide
- **[System Flow](SYSTEM_FLOW_DIAGRAM.md)** - Architecture diagrams

## ğŸ¯ Best Practices

1. **Collect Diverse Feedback**
   - Encourage both positive and negative ratings
   - Aim for balanced feedback (not all ğŸ‘ or all ğŸ‘)

2. **Monitor Performance**
   - Check `/feedback/stats` regularly
   - Watch logs for reward model updates

3. **Tune Settings**
   - Start with default `n_candidates: 4`
   - Adjust based on response time vs quality needs

4. **Manage Costs**
   - RL mode makes 4x API calls
   - Toggle RL off for cost-sensitive scenarios

5. **Clean Old Data**
   - Periodically remove old feedback (90+ days)
   - Prevents database bloat

## ğŸš€ Deployment

### Production Considerations

1. **Environment Variables**
   - Set `ENVIRONMENT=PROD` in `.env`
   - Use production-grade API keys

2. **Database**
   - Consider PostgreSQL for production
   - Implement backup strategy

3. **Caching**
   - Add Redis for response caching
   - Cache model predictions

4. **Load Balancing**
   - Use Gunicorn or uWSGI for Flask
   - Multiple worker processes

5. **Monitoring**
   - Set up logging to file
   - Add error tracking (Sentry, etc.)

### Build for Production

```bash
# Build React app
npm run build

# Serve with production server
# (e.g., Nginx, Apache, or serve package)
```

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ™ Acknowledgments

- **Groq** - Fast LLM inference
- **Sentence Transformers** - Text embeddings
- **Create React App** - Frontend scaffolding
- **Scikit-learn** - Machine learning
- **Flask** - Web framework

## ğŸ“ Support

For issues and questions:
- Open an issue on GitHub
- Check the documentation in the `docs/` folder
- Review the troubleshooting section above

## ğŸ”„ Recent Updates (v2.0)

- âœ¨ Added Reinforcement Learning system
- âœ¨ User feedback collection (thumbs up/down)
- âœ¨ Multi-candidate response generation
- âœ¨ Reward model for quality scoring
- âœ¨ RL mode toggle in UI
- âœ¨ SQLite feedback database
- âœ¨ Comprehensive logging system
- âœ¨ Centralized YAML configuration
- ğŸ“š Complete documentation suite
- ğŸ§ª Test suite included

---

**Built with â¤ï¸ for better financial decision-making**
